# ssh to ec2
$ ssh ec2-user@EC2_INSTANCE

# install Jenkins
sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key
sudo yum install jenkins java-1.8.0 -y
sudo yum remove java-1.7.0-openjdk
sudo service jenkins start

Remember to add user jenkins to local ec2 group docker !

# Setup Jenkins
1. add git plugin and set git credentials (personal github token)
2. add plugin 'Force locale en-US'
3. create pipeline - use jenkins-pipeline/jenkins.groovy

# install docker-compose on ec2
Just download it 

# create docker registry on AWS (ECR)
TODO: describe this step!

# ECR Image
My docker registry is: 677143160410.dkr.ecr.eu-central-1.amazonaws.com/
My image: 
677143160410.dkr.ecr.eu-central-1.amazonaws.com/cloud-demo/app-flask1

# Start EKS cluster:

$ eksctl create cluster  --alb-ingress-access  -f eksworkshop-kubeflow_t2micro.yml

# run flask1 on Kubernetes
$ kubectl apply -f jenkins-pipeline\flask1-deployment-latest.yaml

# Scale EKS cluster
$ kubectl get nodes
$ eksctl get nodegroup --cluster eksworkshop-eksctl
$ eksctl scale nodegroup --cluster=eksworkshop-eksctl --name=micro-nodegroup --nodes=3 

# add load balancer (Amazon ELB)
One has to first deploy flask1 to cluster, otherwise there is nothing to expose, right?

$ kubectl expose deployment flask1-deployment --type=LoadBalancer --port=8888 --target-port=5000
$ kubectl get svc|grep ^flask1  ## ExternalIP

# delete load balancer
$ kubectl delete service flask1-deployment

# Delete EKS cluster:
(will require kubectl delete on services like load balancer first)

$ eksctl delete cluster --name eksworkshop-eksctl


# Docker login
docker login command is generated by aws tool:
$(aws ecr get-login --no-include-email)

# connect AWS IAM to Kubernetes
1. Run jenkins-pipeline/jenkins-aws-create-iam.sh - this creates /usr/local/bin/jenkins_aws_creds.sh
$ sudo bash jenkins-aws-create-iam.sh
2. Add those credentials to Jenkins as secret text under names jenkins-aws-secret-key-id and jenkins-aws-secret-access-key. URI: http://ec2-54-93-180-196.eu-central-1.compute.amazonaws.com:8080/credentials/store/system/ - hover on domain name, click, Add credentials. Holly UX designer, were you drunk?
3. define a k8s user called jenkins, and map to itâ€™s IAM user counterpart
cat << EoF > ~/tmp/jenkins-aws-auth.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapUsers: |
    - userarn: arn:aws:iam::${ACCOUNT_ID}:user/jenkins
      username: jenkins
EoF
4. this maps user jenkins in Kubernetes to AWS Jenkins Account, specified by userarn
kubectl apply -f ~/tmp/jenkins-aws-auth.yaml
5. Crete role able to deploy pods and bind it to kubernetes user jenkins. At this point we have AWS IAM mapped to kubernetes user. Therefore it is enough to set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (see pt 1) to gain access to kubernetes.

$ give_jenkins_rights_on_cluster.sh

# copy ~/.kube/ to jenkins
$ sudo cp -r ~/.kube/ /var/lib/jenkins/

This copies cluster name and config, so when jenkins server is running kubectl, it addresses our cluster. This should be probably solved better, but AWS is so dynamic..




